{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_network(nn.Module):\n",
    "    # MLP softmax output\n",
    "    def __init__(self, state_size, hidden_list, action_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for layer in hidden_list:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, action_size))                      \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "        \n",
    "class value_network(nn.Module):\n",
    "    # MLP no softmax on output\n",
    "    def __init__(self, state_size, hidden_list, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for layer in hidden_list:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, output_size))     \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class transition_network(nn.Module):\n",
    "    # MLP with forward conditioned on action; output of size of input\n",
    "    def __init__(self, state_size, hidden_list, action_size, output_size):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for idx, layer in enumerate(hidden_list):\n",
    "            if idx == 0:\n",
    "                if action_size == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+action_size, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, output_size))\n",
    "        \n",
    "    def forward(self, x, a, eval_mode=False):\n",
    "        if self.action_dim == 2:\n",
    "            out = torch.cat((x, a), 1)\n",
    "        if self.action_dim != 2:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            out = torch.cat((x, extra), 1)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        if eval_mode is True:\n",
    "            out = torch.cat([out, x[:,:3*self.output_size]], 1)\n",
    "        return out\n",
    "\n",
    "class reward_network(nn.Module):\n",
    "    # MLP with forward conditioned on action; output of size 1\n",
    "    def __init__(self, state_size, hidden_list, action_size):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for idx, layer in enumerate(hidden_list):\n",
    "            if idx == 0:\n",
    "                if action_size == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+action_size, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, 1))   \n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        if self.action_dim == 2:\n",
    "            x = torch.cat((x, a), 1)\n",
    "        else:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            x = torch.cat((x, extra), 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, state, hidden, latent):\n",
    "        super(VAE, self).__init__()\n",
    "        self.e1 = nn.Linear(state, hidden)\n",
    "        self.e21 = nn.Linear(hidden, latent)\n",
    "        self.e22 = nn.Linear(hidden, latent)\n",
    "        self.d1 = nn.Linear(latent, hidden)\n",
    "        self.d2 = nn.Linear(hidden, state)\n",
    "        self.latent = latent\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = F.relu(self.e1(x))\n",
    "        mu = self.e21(out)\n",
    "        logvar = self.e22(out)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(mu.size()).normal_()\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = F.relu(self.d1(z))\n",
    "        out = torch.sigmoid(self.d2(out))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, encoding_only=False, training=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if training==True:\n",
    "            z = self.reparametrize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        decoded = self.decode(z)\n",
    "        if encoding_only==False:\n",
    "            return decoded, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "def vae_loss(decoded_, x, mu, logvar, beta=1):\n",
    "    loss_r = nn.BCELoss(size_average=False)\n",
    "    l_1 = loss_r(decoded_, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    l_2 = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return l_1 + beta*l_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rollouts_wm(object):\n",
    "    def __init__(self, batch_size, discount, experience_lenght):\n",
    "        self.batch_size = batch_size\n",
    "        self.rollout_memory = []\n",
    "        self.rollout_values = []\n",
    "        self.batch = []\n",
    "        self.discount = discount\n",
    "        self.transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'terminal'))\n",
    "        self.experience_buffer = []\n",
    "        self.experience_lenght = experience_lenght\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def push_to_trajectory(self, state, action, next_state, reward, terminal):\n",
    "        self.trajectory.append(self.transition(state, action, next_state, reward, terminal))\n",
    "    \n",
    "    def monte_carlo(self, value_network=None):\n",
    "        values = []\n",
    "        if value_network is None:\n",
    "            for idx, state in enumerate(self.trajectory):\n",
    "                value = 0 \n",
    "                for idx_, state_ in enumerate(self.trajectory[idx:]):\n",
    "                    value += self.discount**idx_ * state_.reward\n",
    "                values.append(value)\n",
    "        else:\n",
    "            final_state = self.trajectory[-1].next_state\n",
    "            with torch.no_grad():\n",
    "                final_state_val = value_network.forward(final_state)\n",
    "            for idx, state in enumerate(self.trajectory):\n",
    "                value = 0 \n",
    "                for idx_, state_ in enumerate(self.trajectory[idx:]):\n",
    "                    value += self.discount**idx_ * state_.reward\n",
    "                    if (idx + idx_ + 1) == len(self.trajectory):\n",
    "                        value += self.discount**(idx_+1) * final_state_val\n",
    "                values.append(value)\n",
    "        return values\n",
    "    \n",
    "    def push_to_memory(self, value_network=None):\n",
    "        if value_network is None:\n",
    "            values = self.monte_carlo()\n",
    "        else:\n",
    "            values = self.monte_carlo(value_network)\n",
    "        self.rollout_memory = self.rollout_memory + self.trajectory\n",
    "        self.rollout_values = self.rollout_values + values\n",
    "        self.experience_buffer = self.experience_buffer + self.trajectory\n",
    "        if len(self.experience_buffer) > self.experience_lenght:\n",
    "            del self.experience_buffer[:(len(self.experience_buffer)-self.experience_lenght+1)]              \n",
    "\n",
    "    def sample_data(self, policy_mode=True):\n",
    "        if policy_mode is True:\n",
    "            batch = self.rollout_memory[:self.batch_size]\n",
    "            batch = self.transition(*zip(*batch))\n",
    "            self.rollout_memory = self.rollout_memory[self.batch_size:]\n",
    "            values = torch.cat(self.rollout_values[:self.batch_size])\n",
    "            self.rollout_values = self.rollout_values[self.batch_size:]\n",
    "            state = torch.cat(batch.state)\n",
    "            return state, values\n",
    "        if policy_mode is False:\n",
    "            sample_ = random.sample(self.experience_buffer, self.batch_size)\n",
    "            batch = self.transition(*zip(*sample_))\n",
    "            #terminal = torch.cat(batch.terminal)\n",
    "            state = torch.cat(batch.state)\n",
    "            action = torch.cat(batch.action)\n",
    "            reward = torch.cat(batch.reward)\n",
    "            #reward = reward * terminal\n",
    "            new_state = torch.cat(batch.next_state)\n",
    "            return state, action, reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wmpg(object):\n",
    "    def __init__(self, state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, encoder,\n",
    "                 value_updates=3, policy_updates=3, wm_updates=3, imagination_horizon=5, imagination_lambda=0.75,\n",
    "                 wm_memory=2000, policy_lr=0.0025, value_lr=0.0025, transition_lr=0.005, reward_lr=0.005, clip=None, \n",
    "                 scheduler_step=None, termination_length=None, k=None, entropy_coef=None):\n",
    "        self.memory = rollouts_wm(batch_size, discount, wm_memory)\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.transition_net = transition_net\n",
    "        self.reward_net = reward_net\n",
    "        self.o_p = optim.RMSprop(self.policy_net.parameters(), lr=policy_lr, eps=1e-5)\n",
    "        self.o_v = optim.RMSprop(self.value_net.parameters(), lr=value_lr, eps=1e-5)\n",
    "        self.o_t = optim.Adam(self.transition_net.parameters(), lr=transition_lr)\n",
    "        self.o_r = optim.Adam(self.reward_net.parameters(), lr=reward_lr)\n",
    "        self.loss_v = torch.nn.L1Loss()\n",
    "        self.loss_t = torch.nn.L1Loss()\n",
    "        self.loss_r = torch.nn.L1Loss()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount\n",
    "        self.value_updates = int(value_updates)\n",
    "        self.wm_updates = int(wm_updates)\n",
    "        self.policy_updates = int(policy_updates)\n",
    "        self.imagination_horizon = imagination_horizon\n",
    "        self.imagination_lambda = imagination_lambda\n",
    "        self.cum_rewards = 0\n",
    "        self.clip = clip\n",
    "        if scheduler_step is not None:\n",
    "            self.scheduler_t = optim.lr_scheduler.StepLR(self.o_t, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_r = optim.lr_scheduler.StepLR(self.o_r, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_v = optim.lr_scheduler.StepLR(self.o_v, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_p = optim.lr_scheduler.StepLR(self.o_p, step_size=scheduler_step, gamma=0.99)\n",
    "        self.scheduler_step = scheduler_step\n",
    "        self.termination_length = termination_length\n",
    "        self.k = k\n",
    "        self.encoder = encoder\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "    def train_wm(self):\n",
    "        for i in range(self.wm_updates):\n",
    "            states, actions, rewards, new_states = self.memory.sample_data(False)\n",
    "            self.o_t.zero_grad()\n",
    "            transitions = self.transition_net.forward(states, actions)\n",
    "            loss_t_ = self.loss_t(transitions, new_states[:,:16])\n",
    "            loss_t_.backward()\n",
    "            self.o_t.step()\n",
    "            self.o_r.zero_grad()\n",
    "            rewards_ = self.reward_net.forward(states, actions)\n",
    "            loss_r_ = self.loss_r(rewards_, rewards)\n",
    "            loss_r_.backward()\n",
    "            self.o_r.step()\n",
    "            \n",
    "    def train_value(self, states, values):\n",
    "        for i in range(self.value_updates):\n",
    "            self.o_v.zero_grad()\n",
    "            values_ = self.value_net.forward(states)\n",
    "            loss_v_ = self.loss_v(values_, values)\n",
    "            loss_v_.backward()\n",
    "            if self.clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), self.clip)\n",
    "            self.o_v.step()\n",
    "    \n",
    "    def train_policy(self, states):\n",
    "        self.o_p.zero_grad()\n",
    "        probabilities = self.policy_net.forward(states)\n",
    "        q_values = self.imagine_values(states)\n",
    "        if self.imagination_horizon != 1:\n",
    "            baseline = torch.sum(q_values * probabilities.detach(), dim=1).unsqueeze(-1)\n",
    "            expected_values = torch.sum((q_values - baseline) * probabilities, dim=1)\n",
    "        if self.imagination_horizon == 1:\n",
    "            expected_values = torch.sum(q_values * probabilities, dim=1)\n",
    "        loss_pol = -torch.mean(expected_values, dim=0)\n",
    "        if self.entropy_coef is not None:\n",
    "            loss_e = self.entropy_coef * self.entropy(probabilities)\n",
    "            loss_final = loss_pol - loss_e\n",
    "            loss_final.backward()\n",
    "        if self.entropy_coef is None:\n",
    "            loss_pol.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.clip)\n",
    "        self.o_p.step()\n",
    "            \n",
    "    def schedulers_step(self):\n",
    "        if self.scheduler_step is not None:\n",
    "            self.scheduler_t.step()\n",
    "            self.scheduler_r.step()\n",
    "            self.scheduler_v.step()\n",
    "            self.scheduler_p.step()\n",
    "        \n",
    "    def train_networks(self):\n",
    "        state_batch, value_batch = self.memory.sample_data()\n",
    "        for i_g in range(self.policy_updates):    \n",
    "            self.train_wm()\n",
    "            self.train_value(state_batch, value_batch)\n",
    "            if self.k is None:\n",
    "                self.train_policy(state_batch)\n",
    "            if self.k == 1:\n",
    "                self.train_policy_1(state_batch)\n",
    "        self.schedulers_step()\n",
    "            \n",
    "    def imagine_values(self, states):\n",
    "        q_values = torch.zeros((self.batch_size, self.action_size), dtype=torch.float32)\n",
    "        if self.imagination_lambda == 1:\n",
    "            for i in range(self.action_size):\n",
    "                transitions = states\n",
    "                actions = torch.zeros([self.batch_size, 1], dtype=torch.float32)+i\n",
    "                running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    for j in range(self.imagination_horizon):\n",
    "                        reward = self.reward_net.forward(transitions, actions)\n",
    "                        running_rewards += self.discount**j * reward\n",
    "                        transitions = self.transition_net.forward(transitions, actions, True)\n",
    "                        probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                        actions = self.sample_from_matrix(probabilities)\n",
    "                    running_rewards += self.discount**(self.imagination_horizon) * self.value_net.forward(transitions)\n",
    "                q_values[:,i] = running_rewards.squeeze()\n",
    "        if self.imagination_lambda != 1:\n",
    "            for i in range(self.action_size):\n",
    "                transitions = states\n",
    "                actions = torch.zeros([self.batch_size, 1], dtype=torch.float32)+i\n",
    "                running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                running_lambdas = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    for j in range(self.imagination_horizon):\n",
    "                        rewards = self.reward_net.forward(transitions, actions)\n",
    "                        running_rewards += self.discount**j * rewards\n",
    "                        transitions = self.transition_net.forward(transitions, actions, True)\n",
    "                        values = running_rewards + self.discount**(j+1) * self.value_net.forward(transitions)\n",
    "                        if (j+1) == self.imagination_horizon:\n",
    "                            running_lambdas += self.imagination_lambda**(j)*values\n",
    "                        else:\n",
    "                            running_lambdas += (1 - self.imagination_lambda)*self.imagination_lambda**j*values\n",
    "                        probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                        actions = self.sample_from_matrix(probabilities)\n",
    "                q_values[:,i] = running_lambdas.squeeze()\n",
    "        return q_values\n",
    "    \n",
    "    def train_policy_1(self, states):\n",
    "        self.o_p.zero_grad()\n",
    "        probabilities = self.policy_net.forward(states)\n",
    "        actions = self.sample_from_matrix(probabilities.detach().numpy())\n",
    "        log_probabilities = torch.log(probabilities.gather(1, torch.tensor(actions, dtype=torch.int64)))\n",
    "        q_values = self.imagine_values_1(states, actions)\n",
    "        with torch.no_grad():\n",
    "            baseline = self.value_net.forward(states).detach()\n",
    "        expected_values = torch.sum((q_values - baseline) * log_probabilities, dim=1)\n",
    "        loss_pol = -torch.mean(expected_values, dim=0)\n",
    "        if self.entropy_coef is not None:\n",
    "            loss_e = self.entropy_coef * self.entropy(probabilities)\n",
    "            loss_final = loss_pol - loss_e\n",
    "            loss_final.backward()\n",
    "        if self.entropy_coef is None:\n",
    "            loss_pol.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.clip)\n",
    "        self.o_p.step()\n",
    "        \n",
    "    def imagine_values_1(self, states, actions):\n",
    "        q_values = torch.zeros((self.batch_size, 1), dtype=torch.float32)\n",
    "        if self.imagination_lambda == 1:\n",
    "            transitions = states\n",
    "            running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                for j in range(self.imagination_horizon):\n",
    "                    reward = self.reward_net.forward(transitions, actions)\n",
    "                    running_rewards += self.discount**j * reward\n",
    "                    transitions = self.transition_net.forward(transitions, actions, True)\n",
    "                    probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                    actions = self.sample_from_matrix(probabilities)\n",
    "                running_rewards += self.discount**(self.imagination_horizon) * self.value_net.forward(transitions)\n",
    "            q_values[:,0] = running_rewards.squeeze()\n",
    "        if self.imagination_lambda != 1:\n",
    "            transitions = states\n",
    "            running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            running_lambdas = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                for j in range(self.imagination_horizon):\n",
    "                    rewards = self.reward_net.forward(transitions, actions)\n",
    "                    running_rewards += self.discount**j * rewards\n",
    "                    transitions = self.transition_net.forward(transitions, actions, True)\n",
    "                    values = running_rewards + self.discount**(j+1) * self.value_net.forward(transitions)\n",
    "                    if (j+1) == self.imagination_horizon:\n",
    "                        running_lambdas += self.imagination_lambda**(j)*values\n",
    "                    else:\n",
    "                        running_lambdas += (1 - self.imagination_lambda)*self.imagination_lambda**j*values\n",
    "                    probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                    actions = self.sample_from_matrix(probabilities)\n",
    "            q_values[:,0] = running_lambdas.squeeze()\n",
    "        return q_values\n",
    "                \n",
    "    def sample_from_matrix(self, probs):\n",
    "        cumulative = probs.cumsum(axis=1)\n",
    "        uniform_samples = np.random.rand(len(cumulative), 1)\n",
    "        samples = (uniform_samples < cumulative).argmax(axis=1).astype(np.float32)\n",
    "        samples = torch.tensor(samples.reshape(np.shape(samples)[0],1))\n",
    "        return samples.float()\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        state = state[35:195]\n",
    "        state = state[::2,::2,0]\n",
    "        state[state == 144] = 0\n",
    "        state[state == 109] = 0\n",
    "        state[state != 0] = 1\n",
    "        state = self.enlarge_ball(state)\n",
    "        return state.astype(np.float).ravel()\n",
    "    \n",
    "    def enlarge_ball(self, image):\n",
    "        image = np.copy(image)\n",
    "        for i in range(1,79):\n",
    "            for j in range(1,79):\n",
    "                if image[i,j]==1:\n",
    "                    if image[i,j+1]==0 and image[i,j-1]==0:\n",
    "                        if image[i+1,j]==1:\n",
    "                            image[i,j-1] = 1\n",
    "                            image[i+1,j-1] = 1\n",
    "                            image[i,j+1] = 1\n",
    "                            image[i+1,j+1] = 1\n",
    "                            if i!=0:\n",
    "                                image[i-1,j-1:j+2] = 1\n",
    "                                if i!=1:\n",
    "                                    image[i-2,j-1:j+2] = 1\n",
    "                            if i!=78:\n",
    "                                image[i+2,j-1:j+2] = 1\n",
    "                                if i!=77:\n",
    "                                    image[i+3,j-1:j+2] = 1\n",
    "        return image\n",
    "        \n",
    "    def training(self, env, episodes):\n",
    "        results = np.zeros(episodes)\n",
    "        skip_idx = 19\n",
    "        for i in range(episodes):\n",
    "            observation = env.reset()\n",
    "            self.memory.init_episode()\n",
    "            curr_observation = self.preprocess(observation)\n",
    "            curr_observation = torch.tensor(curr_observation, dtype=torch.float32).reshape(1, 6400)\n",
    "            prev_observation1 = torch.zeros((1, self.encoder.latent), dtype=torch.float32)\n",
    "            prev_observation2 = torch.zeros((1, self.encoder.latent), dtype=torch.float32)\n",
    "            prev_observation3 = torch.zeros((1, self.encoder.latent), dtype=torch.float32)\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            with torch.no_grad():\n",
    "                curr_observation = self.encoder.forward(curr_observation, True, False).detach()\n",
    "            state = torch.cat([curr_observation, prev_observation1, prev_observation2, prev_observation3], 1)\n",
    "            while True:\n",
    "                with torch.no_grad():\n",
    "                    probabilities = self.policy_net.forward(state).detach().numpy()\n",
    "                    action = torch.tensor(np.random.choice(self.action_size, p=probabilities.flatten()), dtype=torch.float32).reshape(1,1)\n",
    "                action_translated = action + 2\n",
    "                new_observation, reward, terminal, _ = env.step(int(action_translated.item()))\n",
    "                new_observation = self.preprocess(new_observation)\n",
    "                new_observation = torch.tensor(new_observation, dtype=torch.float32).reshape(1, 6400)\n",
    "                with torch.no_grad():\n",
    "                    new_observation = self.encoder.forward(new_observation, True, False).detach()\n",
    "                new_state = torch.cat([new_observation, curr_observation, prev_observation1, prev_observation2], 1)\n",
    "                episode_reward += reward\n",
    "                steps += 1       \n",
    "                done = torch.zeros([1, 1], dtype=torch.float32) if terminal else torch.ones([1, 1], dtype=torch.float32)\n",
    "                if reward == 1 or reward == -1:\n",
    "                    skip_idx = 0\n",
    "                reward = torch.zeros([1,1], dtype=torch.float32) + reward\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float32).reshape(1, self.state_size)\n",
    "                if skip_idx > 18 or skip_idx == 0:\n",
    "                    self.memory.push_to_trajectory(state, action, new_state, reward, done)\n",
    "                if len(self.memory.rollout_memory) > self.memory.batch_size:\n",
    "                    self.train_networks()\n",
    "                state = new_state\n",
    "                prev_observation3 = prev_observation2\n",
    "                prev_observation2 = prev_observation1\n",
    "                prev_observation1 = curr_observation\n",
    "                curr_observation = new_observation\n",
    "                skip_idx += 1\n",
    "                if terminal:\n",
    "                    results[i] = episode_reward\n",
    "                    self.cum_rewards += episode_reward\n",
    "                    if self.termination_length is not None:\n",
    "                        if steps < self.termination_length:\n",
    "                            self.memory.push_to_memory()\n",
    "                        else:\n",
    "                            self.memory.push_to_memory(self.value_net)\n",
    "                    if self.termination_length is None:\n",
    "                        self.memory.push_to_memory()\n",
    "                    print(\"\\rEp: {} Online reward: {:.2f}; Steps: {}\".format(i + 1, episode_reward, steps), end=\"\")\n",
    "                    break\n",
    "        return np.array(results)\n",
    "    \n",
    "    def entropy(self, p_matrix):\n",
    "        log_probs = torch.log(p_matrix)\n",
    "        entropy = torch.sum(-p_matrix*log_probs, 1)\n",
    "        return torch.mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:262: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Ep: 1 Online reward: -20.00; Steps: 1041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 3 Online reward: -21.00; Steps: 764"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "env.frameskip = 4\n",
    "latent = 16\n",
    "action_size = 2\n",
    "SEED = 10\n",
    "batch_size = 512\n",
    "discount = 0.99\n",
    "hidden_p = [512]\n",
    "hidden_v = [512]\n",
    "hidden_t = [1028]\n",
    "hidden_r = [1028]\n",
    "i_v = 1\n",
    "i_g = 5\n",
    "i_wm = 3\n",
    "horizon = 5\n",
    "lambda_ = 0.99\n",
    "lr_p = 0.001\n",
    "lr_v = 0.001\n",
    "lr_t = 0.002\n",
    "lr_r = 0.002\n",
    "episodes = 1000\n",
    "clip = 1\n",
    "step = 10\n",
    "termination_length = 100\n",
    "k = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "encoder = VAE(6400, 512, latent)\n",
    "encoder.load_state_dict(torch.load('VAE_MLP_512_16'))\n",
    "\n",
    "policy_net = policy_network(latent*4, hidden_p, action_size)\n",
    "value_net = value_network(latent*4, hidden_v, 1)\n",
    "reward_net = reward_network(latent*4, hidden_r, action_size)\n",
    "transition_net = transition_network(latent*4, hidden_t, action_size, latent)\n",
    "agent = wmpg(4*latent, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, encoder, value_updates=i_v, \n",
    "             policy_updates=i_g, wm_updates=i_wm, imagination_horizon=horizon, imagination_lambda=lambda_, wm_memory=5000, policy_lr=lr_p, \n",
    "             value_lr=lr_v, transition_lr=lr_t, reward_lr=lr_r, clip=clip, scheduler_step=step, termination_length=termination_length, k=k, entropy_coef=0.01)\n",
    "results = agent.training(env, episodes)\n",
    "np.savetxt('wmpg_pong.csv', results, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
